{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random \n",
    "import spacy \n",
    "from spacy.training.example import Example \n",
    "from spacy.util import minibatch, compounding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = [\n",
    "    (\"What is the capital of France?\", {\"entities\": [(21, 27, \"LOC\")]}),\n",
    "    (\"When was Albert Einstein born?\", {\"entities\": [(9, 24, \"PERSON\")]}),\n",
    "    (\"Where is the Great Barrier Reef located?\", {\"entities\": [(9, 27, \"LOC\")]}),\n",
    "    (\"Who painted the Mona Lisa?\", {\"entities\": [(10, 20, \"PERSON\")]}),\n",
    "    (\"What is the tallest mountain on Earth?\", {\"entities\": [(9, 31, \"LOC\")]}),\n",
    "    (\"When did the Titanic sink?\", {\"entities\": [(11, 19, \"EVENT\")]}),\n",
    "    (\"Who wrote 'To Kill a Mockingbird'?\", {\"entities\": [(9, 30, \"PERSON\")]}),\n",
    "    (\"Where was Nelson Mandela born?\", {\"entities\": [(10, 24, \"LOC\")]}),\n",
    "    (\"What is the currency of Japan?\", {\"entities\": [(21, 26, \"LOC\")]}),\n",
    "    (\"Who discovered penicillin?\", {\"entities\": [(13, 24, \"PERSON\")]})\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make a Blank Model\n",
    "nlp=spacy.blank('en')\n",
    "# Add Pipeline in model\n",
    "ner=nlp.add_pipe(\"ner\",last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extract Annotation form Data\n",
    "for _,annotaion in train_data:\n",
    "    ner.add_label(annotaion.get(\"entities\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LOC\n",
      "PERSON\n",
      "LOC\n",
      "PERSON\n",
      "LOC\n",
      "EVENT\n",
      "PERSON\n",
      "LOC\n",
      "LOC\n",
      "PERSON\n"
     ]
    }
   ],
   "source": [
    "# print Annotation\n",
    "for _,annotaion in train_data:\n",
    "    print(annotaion.get(\"entities\")[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<thinc.optimizers.Optimizer at 0x212ffc9cc20>"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Begin training, it's a flag for begin training \n",
    "nlp.begin_training()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select pipeline for training \n",
    "exception_pipe=['ner','wordpiecer','trf_tok2vec']\n",
    "other_pipe=[pipe for pipe in nlp.pipe_names if pipe not in exception_pipe]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "other_pipe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\Mohamed orabi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"What is the currency of Japan?\" with entities \"[(21, 26, 'LOC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mohamed orabi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Who wrote 'To Kill a Mockingbird'?\" with entities \"[(9, 30, 'PERSON')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mohamed orabi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"What is the capital of France?\" with entities \"[(21, 27, 'LOC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mohamed orabi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"When did the Titanic sink?\" with entities \"[(11, 19, 'EVENT')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mohamed orabi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Who painted the Mona Lisa?\" with entities \"[(10, 20, 'PERSON')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mohamed orabi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Where is the Great Barrier Reef located?\" with entities \"[(9, 27, 'LOC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mohamed orabi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"Who discovered penicillin?\" with entities \"[(13, 24, 'PERSON')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n",
      "c:\\Users\\Mohamed orabi\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\spacy\\training\\iob_utils.py:149: UserWarning: [W030] Some entities could not be aligned in the text \"What is the tallest mountain on Earth?\" with entities \"[(9, 31, 'LOC')]\". Use `spacy.training.offsets_to_biluo_tags(nlp.make_doc(text), entities)` to check the alignment. Misaligned entities ('-') will be ignored during training.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Losses {'ner': 3.188767093852145e-07}\n",
      "Losses {'ner': 3.2088691142180126e-09}\n",
      "Losses {'ner': 1.0262569851168118e-08}\n",
      "Losses {'ner': 1.8219172609961542e-08}\n",
      "Losses {'ner': 1.257724893995126e-07}\n",
      "Losses {'ner': 1.0994501778927382e-07}\n",
      "Losses {'ner': 2.2206656509036723e-08}\n",
      "Losses {'ner': 1.276041734615888e-05}\n",
      "Losses {'ner': 1.1219045616497221e-07}\n",
      "Losses {'ner': 1.5142949835257428e-08}\n",
      "Losses {'ner': 0.001859762440651287}\n",
      "Losses {'ner': 1.913018388407383e-05}\n",
      "Losses {'ner': 2.550656207878206e-07}\n",
      "Losses {'ner': 4.2877018901693874e-06}\n",
      "Losses {'ner': 9.696792922314172e-09}\n",
      "Losses {'ner': 8.32486573277127e-06}\n",
      "Losses {'ner': 4.8281429268228165e-08}\n",
      "Losses {'ner': 1.0635010123205135e-08}\n",
      "Losses {'ner': 7.001813898779862e-08}\n",
      "Losses {'ner': 4.2142613582242045e-08}\n",
      "Losses {'ner': 6.199082320290504e-07}\n",
      "Losses {'ner': 8.275497297602555e-07}\n",
      "Losses {'ner': 5.7855922597637933e-08}\n",
      "Losses {'ner': 1.5168969782162793e-09}\n",
      "Losses {'ner': 0.00018983167526688603}\n",
      "Losses {'ner': 4.707256128298956e-09}\n",
      "Losses {'ner': 0.10596080399420603}\n",
      "Losses {'ner': 7.146872593298373e-09}\n",
      "Losses {'ner': 9.198312952368336e-09}\n",
      "Losses {'ner': 1.8185606103650514e-08}\n",
      "Losses {'ner': 1.6784220375881757e-10}\n",
      "Losses {'ner': 4.849830784257866e-10}\n",
      "Losses {'ner': 4.636031194572077e-08}\n",
      "Losses {'ner': 0.0007794356824992717}\n",
      "Losses {'ner': 2.9016679398184993e-06}\n",
      "Losses {'ner': 3.779243614584605e-09}\n",
      "Losses {'ner': 0.0010473368921908558}\n",
      "Losses {'ner': 1.2024339809473168e-07}\n",
      "Losses {'ner': 1.1859836606678216}\n",
      "Losses {'ner': 0.04947830328814324}\n",
      "Losses {'ner': 2.0318437963743605}\n",
      "Losses {'ner': 0.04515453958884566}\n",
      "Losses {'ner': 6.633721001600068e-05}\n",
      "Losses {'ner': 0.0003782247897037686}\n",
      "Losses {'ner': 1.5864148857056575e-06}\n",
      "Losses {'ner': 0.00029212366971370787}\n",
      "Losses {'ner': 5.150123601181918e-06}\n",
      "Losses {'ner': 6.257234190134975e-08}\n",
      "Losses {'ner': 1.8339989285089668e-08}\n",
      "Losses {'ner': 2.927652121281438e-09}\n",
      "Losses {'ner': 1.1111073233022466e-05}\n",
      "Losses {'ner': 1.3513161807461065e-08}\n",
      "Losses {'ner': 4.4737301223632784e-12}\n",
      "Losses {'ner': 9.264035272354463e-08}\n",
      "Losses {'ner': 8.986535659773353e-05}\n",
      "Losses {'ner': 5.148695870218917e-09}\n",
      "Losses {'ner': 9.105137116840695e-11}\n",
      "Losses {'ner': 7.717911011037516e-09}\n",
      "Losses {'ner': 6.486739628600089e-10}\n",
      "Losses {'ner': 1.9909579751763317e-11}\n"
     ]
    }
   ],
   "source": [
    "with nlp.disable_pipes(*other_pipe):\n",
    "    #number of epcohs\n",
    "    for itr in range(60):\n",
    "        # Shuffel Data\n",
    "        random.shuffle(train_data)\n",
    "        losses={}\n",
    "        # Separate data into batches\n",
    "        batches=minibatch(train_data,compounding(4.0,32.0,1.0))\n",
    "        for batch in batches:\n",
    "            for text,annot in batch:\n",
    "                # Make Predication\n",
    "                doc=nlp.make_doc(text)\n",
    "                example=Example.from_dict(doc,annot)\n",
    "                nlp.update([example],\n",
    "                           losses=losses,\n",
    "                           drop=.40)\n",
    "        print(\"Losses\",losses)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Entity [('Albert Einstein', 'PERSON')]\n",
      "Entity [('Nelson Mandela', 'LOC')]\n",
      "Entity []\n",
      "Entity []\n",
      "Entity []\n",
      "Entity []\n",
      "Entity []\n",
      "Entity []\n",
      "Entity []\n",
      "Entity []\n"
     ]
    }
   ],
   "source": [
    "# Test Model\n",
    "for text,_ in train_data:\n",
    "    doc=nlp(text)\n",
    "    print(\"Entity\",[(ent.text,ent.label_)for ent in doc.ents])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
